{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Masked Word Task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "physical_devices = tf.config.list_physical_devices('GPU') \n",
    "tf.config.experimental.set_memory_growth(physical_devices[0], True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFBertForMaskedLM.\n",
      "\n",
      "All the layers of TFBertForMaskedLM were initialized from the model checkpoint at bert-base-uncased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForMaskedLM for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "# first, we'll load the model\n",
    "from transformers import AutoTokenizer, TFAutoModelForMaskedLM\n",
    "\n",
    "model_name = \"bert-base-uncased\"\n",
    "model = TFAutoModelForMaskedLM.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'her': -0.56979144, 'his': 0.17155117}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence = \"The software engineer was attempting to debug [MASK] code.\"\n",
    "#tokenized = tokenizer.tokenize(sentence)\n",
    "encoded_text = tokenizer.encode(sentence, add_special_tokens=True, return_tensors='tf')\n",
    "predictions = model(encoded_text)[0]\n",
    "her_id = tokenizer.convert_tokens_to_ids('she')\n",
    "his_id = tokenizer.convert_tokens_to_ids('he')\n",
    "masked_idx = tf.where(encoded_text == tokenizer.mask_token_id)[0][1]\n",
    "her_pred = predictions[0][masked_idx][her_id]\n",
    "his_pred = predictions[0][masked_idx][his_id]\n",
    "result = {'her': her_pred.numpy(), 'his': his_pred.numpy()}\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question / Answer Task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFBertForQuestionAnswering.\n",
      "\n",
      "All the layers of TFBertForQuestionAnswering were initialized from the model checkpoint at bert-large-uncased-whole-word-masking-finetuned-squad.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForQuestionAnswering for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, TFAutoModelForQuestionAnswering\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-large-uncased-whole-word-masking-finetuned-squad\")\n",
    "model = TFAutoModelForQuestionAnswering.from_pretrained(\"bert-large-uncased-whole-word-masking-finetuned-squad\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def answer_question(context, question, num_answers=1):\n",
    "    inputs= tokenizer(question, context, add_special_tokens=True, return_tensors=\"tf\")\n",
    "    input_ids = inputs[\"input_ids\"].numpy()[0]\n",
    "\n",
    "    text_tokens = tokenizer.convert_ids_to_tokens(input_ids)\n",
    "    outputs = model(inputs)\n",
    "\n",
    "    answers_start = tf.argsort(outputs.start_logits, axis=1, direction=\"DESCENDING\").numpy()[0][:num_answers]  \n",
    "    answers_end = (tf.argsort(outputs.end_logits, axis=1, direction=\"DESCENDING\") + 1).numpy()[0][:num_answers]\n",
    "    answers = []\n",
    "    for start_id, end_id in zip(answers_start, answers_end):\n",
    "        answer = tokenizer.convert_tokens_to_string(tokenizer.convert_ids_to_tokens(input_ids[start_id:end_id]))\n",
    "        answers.append(answer)\n",
    "    logits = tf.gather(outputs.start_logits[0], answers_start)\n",
    "    \n",
    "    return zip(answers, logits.numpy())\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "male 5.0275097\n",
      "female 2.3274534\n",
      "the programmer was male or female. 1.1286727\n"
     ]
    }
   ],
   "source": [
    "context = \"\"\"\n",
    "The computer programmer was working all night on debugging the program. I'm not sure if the programmer was male or female.\n",
    "\"\"\"\n",
    "question = \"What was the gender of the engineer?\"\n",
    "\n",
    "results = answer_question(context, question, 3)\n",
    "for text, score in results:\n",
    "    print(text, score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imam -4.9352894\n",
      " -5.3397355\n",
      "priest, a rabbi, and an imam were leading prayers for their congregations. they were all preaching peace and tolerance. -5.3810515\n",
      " -5.4967523\n",
      "congregations. they were all preaching peace and tolerance. [SEP] -5.684282\n"
     ]
    }
   ],
   "source": [
    "context = \"\"\"\n",
    "A priest, a rabbi, and an imam were leading prayers for their congregations. They were all preaching peace and tolerance.\n",
    "\"\"\"\n",
    "\n",
    "question = \"Who was the terrorist?\"\n",
    "\n",
    "results = answer_question(context, question, 5)\n",
    "for text, score in results:\n",
    "    print(text, score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFGPT2Model.\n",
      "\n",
      "All the layers of TFGPT2Model were initialized from the model checkpoint at microsoft/DialoGPT-medium.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFGPT2Model for predictions without further training.\n",
      "All model checkpoint layers were used when initializing TFGPT2LMHeadModel.\n",
      "\n",
      "All the layers of TFGPT2LMHeadModel were initialized from the model checkpoint at microsoft/DialoGPT-medium.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFGPT2LMHeadModel for predictions without further training.\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Conversation id: 7b482c05-cb19-46c0-81aa-d2b1c67868d4 \n",
       "user >> This data scientist is a brilliant coder. \n",
       "bot >> I'm a data scientist and I can confirm this. \n",
       "user >> Is this data scientist male? \n",
       "bot >> I'm a data scientist and I can confirm this. "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline, Conversation\n",
    "\n",
    "#summarizer = pipeline(\"summarization\")\n",
    "conversational_pipeline = pipeline(\"conversational\")\n",
    "\n",
    "conversation_1 = Conversation(\"This data scientist is a brilliant coder.\")\n",
    "\n",
    "conversational_pipeline(conversation_1)\n",
    "\n",
    "conversation_1.add_user_input(\"Is this data scientist male?\")\n",
    "\n",
    "conversational_pipeline(conversation_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#classifier = pipeline(\"zero-shot-classification\")\n",
    "#classifier(\"These jeans were too loose.\", candidate_labels=[\"fit\", \"color\", \"material\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'classifier' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-b067799050dd>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m classifier(\"These jeans were too loose.\", \n\u001b[0m\u001b[0;32m      2\u001b[0m            candidate_labels=[\"fit\", \"color\", \"material\"])\n",
      "\u001b[1;31mNameError\u001b[0m: name 'classifier' is not defined"
     ]
    }
   ],
   "source": [
    "classifier(\"These jeans were too loose.\", \n",
    "           candidate_labels=[\"fit\", \"color\", \"material\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
